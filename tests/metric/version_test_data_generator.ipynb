{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import copy\n",
    "from metric_base import *\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "def metric_output(metric_class, data, dataloader=None):\n",
    "    name = metric_class._name\n",
    "    version = metric_class._version\n",
    "    if dataloader:\n",
    "        dataloader.all_vocab_list = data['init']['dataloader']['all_vocab_list']\n",
    "        dataloader.valid_vocab_len = data['init']['dataloader']['valid_vocab_len']\n",
    "        dataloader.word2id = dict(zip(range(len(data['init']['dataloader']['all_vocab_list'])), \\\n",
    "                                      data['init']['dataloader']['all_vocab_list']))\n",
    "        data['init']['dataloader'] = dataloader\n",
    "    metric = metric_class(**data['init'])\n",
    "    for batch in data['forward']:\n",
    "        metric.forward(**batch)\n",
    "    return metric.close()\n",
    "\n",
    "def get_vocab(size):\n",
    "    elm = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "    all_vocab_list = [\"<pad>\", \"<unk>\", \"<go>\", \"<eos>\"]\n",
    "    for _ in range(size):\n",
    "        word = []\n",
    "        wlen = np.random.randint(1, 10)\n",
    "        for i in range(wlen):\n",
    "            word.append(np.random.choice(elm)[0])\n",
    "        all_vocab_list.append(\"\".join(word))\n",
    "    return all_vocab_list\n",
    "\n",
    "def get_prec_rec_data(all_vocab_list, valid_vocab_len, num_gen_per_inst):\n",
    "    go_id = 2\n",
    "    eos_id = 3\n",
    "    unk_id = 1\n",
    "    all_vocab_ids = list(range(2)) + list(range(4, len(all_vocab_list)))\n",
    "    valid_vocab_ids = list(range(2)) + list(range(4, valid_vocab_len))\n",
    "    for ref_rng, gen_rng, num_insts in zip([[0, 5], [20, 25], [0, 5], [20, 25]], [[0, 5], [20, 25], [20, 25], [0, 5]], [20] * 4):\n",
    "        reference = []\n",
    "        gen = []\n",
    "        for cid in range(num_insts):\n",
    "            tmp_ref = []\n",
    "            ngrams = set()\n",
    "            for _ in range(np.random.randint(1, 2 * num_gen_per_inst)):\n",
    "                rlen = np.random.randint(*ref_rng)\n",
    "                r = [int(word) for word in np.random.choice(all_vocab_ids, rlen)]\n",
    "                tmp_ref.append([go_id] + r + [eos_id])\n",
    "                for gramlen in range(1, min(rlen + 1, 8)):\n",
    "                    for i in range(rlen - gramlen + 1):\n",
    "                        ngrams.add(tuple(r[i: i + gramlen]))\n",
    "            reference.append(tmp_ref)\n",
    "            ngrams = list(ngrams)\n",
    "            tmp_gen = []\n",
    "            for _ in range(num_gen_per_inst):\n",
    "                g = []\n",
    "                for __ in range(2):\n",
    "                    g += [int(word) for word in np.random.choice(valid_vocab_ids, np.random.randint(0, 3, 1))]\n",
    "                    if ngrams and np.random.rand() < 0.5:\n",
    "                        g += list(ngrams[np.random.choice(list(range(len(ngrams))), 1)[0]])\n",
    "                g += [int(word) for word in np.random.choice(valid_vocab_ids, np.random.randint(0, 3, 1))]\n",
    "                tmp_gen.append(g + [eos_id])\n",
    "            gen.append(tmp_gen)\n",
    "        idx = list(range(len(gen)))\n",
    "        np.random.shuffle(idx)\n",
    "        reference = [reference[i] for i in idx]\n",
    "        gen = [gen[i] for i in idx]\n",
    "        yield reference, gen\n",
    "\n",
    "def get_ref_gen(all_vocab_list, valid_vocab_len, empty_ref, empty_gen):\n",
    "    go_id = 2\n",
    "    eos_id = 3\n",
    "    unk_id = 1\n",
    "    all_vocab_ids = list(range(2)) + list(range(4, len(all_vocab_list)))\n",
    "    valid_vocab_ids = list(range(2)) + list(range(4, valid_vocab_len))\n",
    "    if empty_ref and empty_gen:\n",
    "        return [[[go_id, eos_id]]], [[eos_id]]\n",
    "    if empty_ref and not empty_gen:\n",
    "        return [[[go_id, eos_id]]], [[int(word) for word in np.random.choice(valid_vocab_ids, 10)]]\n",
    "    if not empty_ref and empty_gen:\n",
    "        return [[[go_id] + [int(word) for word in np.random.choice(valid_vocab_ids, 10)] + [eos_id]]], [[eos_id]]\n",
    "    reference = []\n",
    "    gen = []\n",
    "    for rng, num_insts in zip([[0, 5], [20, 25]], [20, 80]):\n",
    "        for _ in range(num_insts):\n",
    "            rlen = np.random.randint(*rng)\n",
    "            r = [int(word) for word in np.random.choice(all_vocab_ids, rlen)]\n",
    "            ngrams = set()\n",
    "            for gramlen in range(1, min(rlen + 1, 8)):\n",
    "                for i in range(rlen - gramlen + 1):\n",
    "                    ngrams.add(tuple(r[i: i + gramlen]))\n",
    "            ngrams = list(ngrams)\n",
    "            g = []\n",
    "            for __ in range(2):\n",
    "                g += [int(word) for word in np.random.choice(valid_vocab_ids, np.random.randint(0, 3, 1))]\n",
    "                if ngrams and np.random.rand() < 0.5:\n",
    "                    g += list(ngrams[np.random.choice(list(range(len(ngrams))), 1)[0]])\n",
    "            g += [int(word) for word in np.random.choice(valid_vocab_ids, np.random.randint(0, 3, 1))]\n",
    "            reference.append([go_id] + r + [eos_id])\n",
    "            gen.append(g + [eos_id])\n",
    "    idx = list(range(len(gen)))\n",
    "    np.random.shuffle(idx)\n",
    "    reference = [reference[i] for i in idx]\n",
    "    gen = [gen[i] for i in idx]\n",
    "    return reference, gen\n",
    "\n",
    "def get_ref_prob_len(all_vocab_list, valid_vocab_len, use_all_vocab):\n",
    "    go_id = 2\n",
    "    eos_id = 3\n",
    "    unk_id = 1\n",
    "    all_vocab_ids = list(range(2)) + list(range(4, len(all_vocab_list)))\n",
    "    valid_vocab_ids = list(range(2)) + list(range(4, valid_vocab_len)) if not use_all_vocab else all_vocab_ids\n",
    "    vocab_size = valid_vocab_len if not use_all_vocab else len(all_vocab_list)\n",
    "    reference = []\n",
    "    gen = []\n",
    "    gen_log_prob = []\n",
    "    for rng, num_insts in zip([[0, 5], [20, 25]], [20, 80]):\n",
    "        for cid in range(num_insts):\n",
    "            rlen = np.random.randint(*rng)\n",
    "            r = [int(word) for word in np.random.choice(all_vocab_ids, rlen)]\n",
    "            ngrams = set()\n",
    "            for gramlen in range(1, min(8, rlen + 1)):\n",
    "                for i in range(rlen - gramlen + 1):\n",
    "                    ngrams.add(tuple(r[i: i + gramlen]))\n",
    "            ngrams = list(ngrams)\n",
    "            g = []\n",
    "            for i, word in enumerate(r):\n",
    "                if np.random.rand() < 0.4:\n",
    "                    g.append(word)\n",
    "                else:\n",
    "                    g.append(int(np.random.choice(all_vocab_ids, 1)[0]))\n",
    "            reference.append([go_id] + r + [eos_id])\n",
    "            g += [eos_id]\n",
    "            gen.append(g)\n",
    "    idx = list(range(len(gen)))\n",
    "    np.random.shuffle(idx)\n",
    "    reference = [reference[i] for i in idx]\n",
    "    gen = [gen[i] for i in idx]\n",
    "\n",
    "    max_gen_len = max([len(g) for g in gen])\n",
    "    for g in gen:\n",
    "        sent_prob = []\n",
    "        for word in g:\n",
    "            if not use_all_vocab and word >= valid_vocab_len:\n",
    "                word = unk_id\n",
    "            dis = np.random.randint(1, 1000, vocab_size)\n",
    "            dis[word] += np.sum(dis)\n",
    "            sent_prob.append(np.log(dis / np.sum(dis)).tolist())\n",
    "        for _ in range(max_gen_len - len(g)):\n",
    "            dis = np.random.randint(1, 1000, vocab_size)\n",
    "            sent_prob.append(np.log(dis / np.sum(dis)).tolist())\n",
    "        gen_log_prob.append(sent_prob)\n",
    "    return reference, gen_log_prob, [len(r) for r in reference]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cotk.metric import AccuracyMetric\n",
    "with open('version_test_data/AccuracyMetric_v1.jsonl', 'w') as file:\n",
    "    obj = {'init': {'dataloader': None, 'label_key': '_label', 'prediction_key': '_prediction'}, \n",
    "          'forward': []}\n",
    "    l = []\n",
    "    p = []\n",
    "    for i, offset in zip(range(10), range(9, -1, -1)):\n",
    "        cnt = offset + 1\n",
    "        l.extend([i] * cnt * 2)\n",
    "        p.extend([i + offset] * cnt)\n",
    "        p.extend([i - offset] * cnt)\n",
    "        print(i, offset, cnt)\n",
    "    idx = list(range(len(l)))\n",
    "    np.random.shuffle(idx)\n",
    "    l = [l[i] for i in idx]\n",
    "    p = [p[i] for i in idx]\n",
    "    mid = len(l) // 2\n",
    "    obj['forward'].append({'data': {'_label': l[:mid], '_prediction': p[:mid]}})\n",
    "    obj['forward'].append({'data': {'_label': l[mid:], '_prediction': p[mid:]}})\n",
    "    accuracy_metric = AccuracyMetric(**obj['init'])\n",
    "    obj['output'] = metric_output(AccuracyMetric, obj, dataloader=None)\n",
    "    file.write(json.dumps(obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cotk.metric import BleuCorpusMetric\n",
    "with open(\"version_test_data/BleuCorpusMetric_v1.jsonl\", \"w\") as file:\n",
    "    for empty_ref in [True, False]:\n",
    "        for empty_hyp in [True, False]:\n",
    "            all_vocab_list = get_vocab(20)\n",
    "            valid_vocab_len = 20\n",
    "            obj = {'init': {'dataloader': {'all_vocab_list': all_vocab_list, 'valid_vocab_len': valid_vocab_len}, \n",
    "                            'ignore_smoothing_error': False, 'reference_allvocabs_key':'_ref_allvocabs', 'gen_key': '_gen'},\n",
    "                  'forward': []}\n",
    "            reference, gen = get_ref_gen(all_vocab_list, valid_vocab_len, empty_ref, empty_hyp)\n",
    "            if len(gen) == 1:\n",
    "                obj['forward'].append({'data': {'_ref_allvocabs': reference, '_gen': gen}})\n",
    "            else:\n",
    "                mid = len(gen) // 2\n",
    "                obj['forward'].append({'data': {'_ref_allvocabs': reference[:mid], '_gen': gen[:mid]}})\n",
    "                obj['forward'].append({'data': {'_ref_allvocabs': reference[mid:], '_gen': gen[mid:]}})\n",
    "            obj['output'] = metric_output(BleuCorpusMetric, copy.deepcopy(obj), FakeDataLoader())\n",
    "            file.write(json.dumps(obj) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cotk.metric import SelfBleuCorpusMetric\n",
    "with open(\"version_test_data/SelfBleuCorpusMetric_v1.jsonl\", \"w\") as file:\n",
    "    for sample in [10, 1000]:\n",
    "        all_vocab_list = get_vocab(20)\n",
    "        valid_vocab_len = 20\n",
    "        obj = {'init': {'dataloader': {'all_vocab_list': all_vocab_list, 'valid_vocab_len': valid_vocab_len}, \n",
    "                        'gen_key': '_gen', 'sample': sample, 'seed': 1229, 'cpu_count': None},\n",
    "              'forward': []}\n",
    "        reference, gen = get_ref_gen(all_vocab_list, valid_vocab_len, False, False)\n",
    "        mid = len(gen) // 2\n",
    "        obj['forward'].append({'data': {'_gen': gen[:mid]}})\n",
    "        obj['forward'].append({'data': {'_gen': gen[mid:]}})\n",
    "        obj['output'] = metric_output(SelfBleuCorpusMetric, copy.deepcopy(obj), FakeDataLoader())\n",
    "        file.write(json.dumps(obj) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cotk.metric import FwBwBleuCorpusMetric\n",
    "with open(\"version_test_data/FwBwBleuCorpusMetric_v1.jsonl\", \"w\") as file:\n",
    "    for test_sz, gen_sz in zip([50, 100], [100, 50]):\n",
    "        for sample in [25, 75, 125]:\n",
    "            all_vocab_list = get_vocab(20)\n",
    "            valid_vocab_len = 20\n",
    "            reference, gen = get_ref_gen(all_vocab_list, valid_vocab_len, False, False)\n",
    "            reference = reference[:test_sz]\n",
    "            gen = gen[:gen_sz]\n",
    "            obj = {'init': {'dataloader': {'all_vocab_list': all_vocab_list, 'valid_vocab_len': valid_vocab_len},\n",
    "                            'reference_test_list': reference,\n",
    "                            'gen_key': '_gen', 'sample': sample, 'seed': 1229, 'cpu_count': None},\n",
    "                  'forward': []}\n",
    "            mid = len(gen) // 2\n",
    "            obj['forward'].append({'data': {'_gen': gen[:mid]}})\n",
    "            obj['forward'].append({'data': {'_gen': gen[mid:]}})\n",
    "            obj['output'] = metric_output(FwBwBleuCorpusMetric, copy.deepcopy(obj), FakeDataLoader())\n",
    "            file.write(json.dumps(obj) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cotk.metric import MultiTurnBleuCorpusMetric\n",
    "with open(\"version_test_data/MultiTurnBleuCorpusMetric_v1.jsonl\", \"w\") as file:\n",
    "    all_vocab_list = get_vocab(20)\n",
    "    valid_vocab_len = 20\n",
    "    reference, _gen = get_ref_gen(all_vocab_list, valid_vocab_len, False, False)\n",
    "    turn_length = []\n",
    "    turn = []\n",
    "    gen = []\n",
    "    s = 0\n",
    "    while s < len(reference):\n",
    "        l = np.random.randint(1, min(10, len(reference) - s + 1))\n",
    "        turn_length.append(l)\n",
    "        turn.append(reference[s: s + l])\n",
    "        gen.append(_gen[s: s + l])\n",
    "        s += l\n",
    "    obj = {'init': {'dataloader': {'all_vocab_list': all_vocab_list, 'valid_vocab_len': valid_vocab_len},\n",
    "                   'ignore_smoothing_error': True,\n",
    "                   'multi_turn_reference_allvocabs_key': '_reference_allvocabs',\n",
    "                   'multi_turn_gen_key': '_multi_turn_gen',\n",
    "                   'turn_len_key': '_turn_length'},\n",
    "          'forward': []}\n",
    "    mid = len(turn) // 2\n",
    "    obj['forward'].append({'data': {'_reference_allvocabs': turn[:mid], '_multi_turn_gen': gen[:mid], '_turn_length': turn_length[:mid]}})\n",
    "    obj['forward'].append({'data': {'_reference_allvocabs': turn[mid:], '_multi_turn_gen': gen[mid:], '_turn_length': turn_length[mid:]}})\n",
    "    obj['output'] = metric_output(MultiTurnBleuCorpusMetric, copy.deepcopy(obj), FakeMultiDataloader())\n",
    "    file.write(json.dumps(obj) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cotk.metric import PerplexityMetric\n",
    "with open(\"version_test_data/PerplexityMetric_v1.jsonl\", \"w\") as file:\n",
    "    for use_all_vocab in [True, False]:\n",
    "        all_vocab_list = get_vocab(20)\n",
    "        valid_vocab_len = 20\n",
    "        reference, gen_log_prob, gen_len = get_ref_prob_len(all_vocab_list, valid_vocab_len, use_all_vocab)\n",
    "        obj = {'init': {'dataloader': {'all_vocab_list': all_vocab_list, 'valid_vocab_len': valid_vocab_len}, \n",
    "                       'reference_allvocabs_key': '_ref_allvocabs', \n",
    "                        'reference_len_key': '_ref_length', \n",
    "                       'gen_log_prob_key': '_gen_log_prob',\n",
    "                       'invalid_vocab': use_all_vocab,\n",
    "                       'full_check': False},\n",
    "              'forward': []}\n",
    "        mid = len(reference) // 2\n",
    "        obj['forward'].append({'data': {'_ref_allvocabs': reference[:mid], '_ref_length': gen_len[:mid], '_gen_log_prob': gen_log_prob[:mid]}})\n",
    "        obj['forward'].append({'data': {'_ref_allvocabs': reference[mid:], '_ref_length': gen_len[mid:], '_gen_log_prob': gen_log_prob[mid:]}})\n",
    "        obj['output'] = metric_output(PerplexityMetric, copy.deepcopy(obj), FakeDataLoader())\n",
    "        file.write(json.dumps(obj) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cotk.metric import MultiTurnPerplexityMetric\n",
    "with open(\"version_test_data/MultiTurnPerplexityMetric_v1.jsonl\", \"w\") as file:\n",
    "    for use_all_vocab in [True, False]:\n",
    "        all_vocab_list = get_vocab(20)\n",
    "        valid_vocab_len = 20\n",
    "        _reference, _gen_log_prob, _gen_len = get_ref_prob_len(all_vocab_list, valid_vocab_len, use_all_vocab)\n",
    "        reference = []\n",
    "        gen_log_prob = []\n",
    "        gen_len = []\n",
    "        s = 0\n",
    "        while s < len(_reference):\n",
    "            l = np.random.randint(1, min(10, len(_reference) - s + 1))\n",
    "            gen_len.append(_gen_len[s: s + l])\n",
    "            reference.append(_reference[s: s + l])\n",
    "            gen_log_prob.append(_gen_log_prob[s: s + l])\n",
    "            if l > 3:\n",
    "                gen_len[-1][1] = 0\n",
    "            s += l\n",
    "        obj = {'init': {'dataloader': {'all_vocab_list': all_vocab_list, 'valid_vocab_len': valid_vocab_len}, \n",
    "                       'multi_turn_reference_allvocabs_key': '_multi_turn_ref_allvocabs', \n",
    "                        'multi_turn_reference_len_key': '_multi_turn_ref_length', \n",
    "                       'multi_turn_gen_log_prob_key': '_multi_turn_gen_log_prob',\n",
    "                       'invalid_vocab': use_all_vocab,\n",
    "                       'full_check': False},\n",
    "              'forward': []}\n",
    "        mid = len(reference) // 2\n",
    "        obj['forward'].append({'data': {'_multi_turn_ref_allvocabs': reference[:mid], '_multi_turn_ref_length': gen_len[:mid], '_multi_turn_gen_log_prob': gen_log_prob[:mid]}})\n",
    "        obj['forward'].append({'data': {'_multi_turn_ref_allvocabs': reference[mid:], '_multi_turn_ref_length': gen_len[mid:], '_multi_turn_gen_log_prob': gen_log_prob[mid:]}})\n",
    "        obj['output'] = metric_output(MultiTurnPerplexityMetric, copy.deepcopy(obj), FakeMultiDataloader())\n",
    "        file.write(json.dumps(obj) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cotk.metric import NgramFwBwPerplexityMetric\n",
    "with open(\"version_test_data/NgramFwBwPerplexityMetric_v1.jsonl\", \"w\") as file:\n",
    "    for ngram in range(1, 5):\n",
    "        all_vocab_list = get_vocab(20)\n",
    "        valid_vocab_len = 20\n",
    "        reference, gen = get_ref_gen(all_vocab_list, valid_vocab_len, False, False)\n",
    "        obj = {'init': {'dataloader': {'all_vocab_list': all_vocab_list, 'valid_vocab_len': valid_vocab_len}, \n",
    "                        'ngram': ngram,\n",
    "                        'reference_test_list': reference, 'gen_key': '_gen', 'cpu_count': None},\n",
    "              'forward': []}\n",
    "        mid = len(gen) // 2\n",
    "        obj['forward'].append({'data': {'_gen': gen[:mid]}})\n",
    "        obj['forward'].append({'data': {'_gen': gen[mid:]}})\n",
    "        obj['output'] = metric_output(NgramFwBwPerplexityMetric, copy.deepcopy(obj), FakeDataLoader())\n",
    "        file.write(json.dumps(obj) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cotk.metric import BleuPrecisionRecallMetric\n",
    "with open(\"version_test_data/BleuPrecisionRecallMetric_v1.jsonl\", \"w\") as file:\n",
    "    for ngram in range(1, 4):\n",
    "        all_vocab_list = get_vocab(20)\n",
    "        valid_vocab_len = 20\n",
    "        for reference, gen in get_prec_rec_data(all_vocab_list, valid_vocab_len, 3):\n",
    "            obj = {'init': {'dataloader': {'all_vocab_list': all_vocab_list, 'valid_vocab_len': valid_vocab_len},\n",
    "                           'ngram': ngram, 'generated_num_per_context': 3, 'candidates_allvocabs_key': '_candidate_allvocabs',\n",
    "                           'multiple_gen_key': '_multiple_gen'},\n",
    "                  'forward': []}\n",
    "            mid = len(gen) // 2\n",
    "            obj['forward'].append({'data': {'_candidate_allvocabs': reference[:mid], '_multiple_gen': gen[:mid]}})\n",
    "            obj['forward'].append({'data': {'_candidate_allvocabs': reference[mid:], '_multiple_gen': gen[mid:]}})\n",
    "            obj['output'] = metric_output(BleuPrecisionRecallMetric, copy.deepcopy(obj), FakeMultiDataloader())\n",
    "            file.write(json.dumps(obj) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cotk.metric import EmbSimilarityPrecisionRecallMetric\n",
    "with open(\"version_test_data/EmbSimilarityPrecisionRecallMetric_v1.jsonl\", \"w\") as file:\n",
    "    for mode in ['avg', 'extrema']:\n",
    "        all_vocab_list = get_vocab(20)\n",
    "        valid_vocab_len = 20\n",
    "        word2vec = {}\n",
    "        for word in all_vocab_list[2: valid_vocab_len] + [all_vocab_list[0]]:\n",
    "            emb = (np.random.choice(list(range(3)), 1)[0] + np.random.rand(10)).tolist()\n",
    "            word2vec[word] = emb\n",
    "        for reference, gen in get_prec_rec_data(all_vocab_list, valid_vocab_len, 3):\n",
    "            obj = {'init': {'dataloader': {'all_vocab_list': all_vocab_list, 'valid_vocab_len': valid_vocab_len},\n",
    "                           'word2vec': word2vec, 'mode': mode, 'generated_num_per_context': 3, 'candidates_allvocabs_key': '_candidate_allvocabs',\n",
    "                           'multiple_gen_key': '_multiple_gen'},\n",
    "                  'forward': []}\n",
    "            mid = len(gen) // 2\n",
    "            obj['forward'].append({'data': {'_candidate_allvocabs': reference[:mid], '_multiple_gen': gen[:mid]}})\n",
    "            obj['forward'].append({'data': {'_candidate_allvocabs': reference[mid:], '_multiple_gen': gen[mid:]}})\n",
    "            obj['output'] = metric_output(EmbSimilarityPrecisionRecallMetric, copy.deepcopy(obj), FakeMultiDataloader())\n",
    "            file.write(json.dumps(obj) + \"\\n\")\n",
    "    # empty gen\n",
    "    obj = {'init': {'dataloader': {'all_vocab_list': all_vocab_list, 'valid_vocab_len': valid_vocab_len},\n",
    "                           'word2vec': word2vec, 'mode': mode, 'generated_num_per_context': 3, 'candidates_allvocabs_key': '_candidate_allvocabs',\n",
    "                           'multiple_gen_key': '_multiple_gen'},\n",
    "                  'forward': [{'data': {'_candidate_allvocabs': reference[:1], '_multiple_gen': [[[3]] * 3]}}]}\n",
    "    obj['output'] = metric_output(EmbSimilarityPrecisionRecallMetric, copy.deepcopy(obj), FakeMultiDataloader())\n",
    "    file.write(json.dumps(obj) + \"\\n\")\n",
    "    # empty ref\n",
    "    obj = {'init': {'dataloader': {'all_vocab_list': all_vocab_list, 'valid_vocab_len': valid_vocab_len},\n",
    "                           'word2vec': word2vec, 'mode': mode, 'generated_num_per_context': 3, 'candidates_allvocabs_key': '_candidate_allvocabs',\n",
    "                           'multiple_gen_key': '_multiple_gen'},\n",
    "                  'forward': [{'data': {'_candidate_allvocabs': [[[2, 3]]], '_multiple_gen': gen[:1]}}]}\n",
    "    obj['output'] = metric_output(EmbSimilarityPrecisionRecallMetric, copy.deepcopy(obj), FakeMultiDataloader())\n",
    "    file.write(json.dumps(obj) + \"\\n\")\n",
    "    # empty ref & gen\n",
    "    obj = {'init': {'dataloader': {'all_vocab_list': all_vocab_list, 'valid_vocab_len': valid_vocab_len},\n",
    "                           'word2vec': word2vec, 'mode': mode, 'generated_num_per_context': 3, 'candidates_allvocabs_key': '_candidate_allvocabs',\n",
    "                           'multiple_gen_key': '_multiple_gen'},\n",
    "                  'forward': [{'data': {'_candidate_allvocabs': [[[2, 3]]], '_multiple_gen': [[[3]] * 3]}}]}\n",
    "    obj['output'] = metric_output(EmbSimilarityPrecisionRecallMetric, copy.deepcopy(obj), FakeMultiDataloader())\n",
    "    file.write(json.dumps(obj) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
